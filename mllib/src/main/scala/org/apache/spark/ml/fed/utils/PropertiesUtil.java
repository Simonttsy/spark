package org.apache.spark.ml.fed.utils;import org.apache.commons.io.output.ByteArrayOutputStream;import org.apache.hadoop.conf.Configuration;import org.apache.hadoop.fs.FSDataInputStream;import org.apache.hadoop.fs.FileStatus;import org.apache.hadoop.fs.FileSystem;import org.apache.hadoop.fs.Path;import org.apache.hadoop.io.IOUtils;import java.io.ByteArrayInputStream;import java.io.FileInputStream;import java.io.IOException;import java.io.InputStream;import java.net.URI;import java.util.ArrayList;import java.util.Properties;public class PropertiesUtil {    private static InputStream inputStream = null;    private static Properties PROPERTIES = new Properties();    //hdfs://sit-poc1.novalocal:8020/test/t_dir/arbiter/guest.properties    private static InputStream readFileByHdfs(String hdfsPath) throws Exception {        ByteArrayOutputStream stream = null;        FSDataInputStream fsDataInputStream = null;        try {            Configuration conf = new Configuration();            conf.set("dfs.client.use.datanode.hostname", "true");            FileSystem fs = FileSystem.get(URI.create(hdfsPath), conf);            fsDataInputStream = fs.open(new Path(hdfsPath));            stream = new ByteArrayOutputStream();            IOUtils.copyBytes(fsDataInputStream, stream, 1024);        } catch (Exception e) {            System.out.println("readFileByHdfs error: " + e.toString());//            Logger.error("readFileByHdfs error,{}",e);        } finally {            if (fsDataInputStream != null) {                IOUtils.closeStream(fsDataInputStream);            }        }        return new ByteArrayInputStream(stream.toByteArray());    }    //hdfs://sit-poc1.novalocal:8020/test/t_dir/arbiter    public static ArrayList<Properties> readHDFSDir(String hdfsDirPath) throws Exception {        ByteArrayOutputStream stream;        FSDataInputStream fsDataInputStream = null;        ArrayList<Properties> pros = new ArrayList<>();        try {            Configuration conf = new Configuration();            conf.set("dfs.client.use.datanode.hostname", "true");            FileSystem fs = FileSystem.get(URI.create(hdfsDirPath), conf);            FileStatus[] stats = fs.listStatus(new Path(hdfsDirPath));            for (FileStatus stat : stats) {                //打印每个文件路径                System.out.println(stat.getPath().toString());                fsDataInputStream = fs.open(stat.getPath());                stream = new ByteArrayOutputStream();                IOUtils.copyBytes(fsDataInputStream, stream, 1024);                ByteArrayInputStream inputStream = new ByteArrayInputStream(stream.toByteArray());                Properties properties = new Properties();                properties.load(inputStream);                pros.add(properties);            }        } catch (Exception e) {            System.out.println("readFileByHdfs error: " + e.toString());//            Logger.error("readFileByHdfs error,{}",e);        } finally {            if (fsDataInputStream != null) {                IOUtils.closeStream(fsDataInputStream);            }        }        return pros;    }    public static Properties getHdfsPropertie(String hdfsPath) throws Exception {        if (inputStream == null) {            inputStream = readFileByHdfs(hdfsPath);            PROPERTIES.load(inputStream);        }        return PROPERTIES;    }    private static Properties getLocalProperties(String path) {        try {            PROPERTIES.load(new FileInputStream(path));        } catch (IOException e) {            e.printStackTrace();        }        return PROPERTIES;    }    public static Properties getProperties(String path) throws Exception {        if (path.startsWith("hdfs:/")) {            return getHdfsPropertie(path);        } else return getLocalProperties(path);    }}